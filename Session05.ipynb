{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEJ45tll/YkK6U3Xz5EMfE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rachelllle/Spark-Core/blob/main/Session05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ylh_3t9qFdFV"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"DataFrame\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, \"Alice\",   \"FR\", 100),\n",
        "    (2, \"Bob\",     \"FR\", 200),\n",
        "    (3, \"Charlie\", \"UK\", 150),\n",
        "    (4, \"David\",   \"FR\",  50),\n",
        "    (5, \"Eve\",     \"UK\", 300)\n",
        "]\n",
        "\n",
        "# Create DataFrame with column names\n",
        "df = spark.createDataFrame(data, [\"id\", \"name\", \"country\", \"amount\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyzV6axeH7xn",
        "outputId": "45c21850-dff9-4b5f-b682-d5731656a217"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+-------+------+\n",
            "| id|   name|country|amount|\n",
            "+---+-------+-------+------+\n",
            "|  1|  Alice|     FR|   100|\n",
            "|  2|    Bob|     FR|   200|\n",
            "|  3|Charlie|     UK|   150|\n",
            "|  4|  David|     FR|    50|\n",
            "|  5|    Eve|     UK|   300|\n",
            "+---+-------+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjvC2Q1WHUIb",
        "outputId": "dbdc719c-6b0b-4ce0-e158-383a4249b3f6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- amount: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "# Define the schema explicitly\n",
        "schema = StructType([\n",
        "    StructField(\"id\",      IntegerType(), nullable=False),\n",
        "    StructField(\"name\",    StringType(),  nullable=False),\n",
        "    StructField(\"country\", StringType(),  nullable=False),\n",
        "    StructField(\"amount\",  IntegerType(), nullable=False)\n",
        "])\n",
        "\n",
        "# Create DataFrame with explicit schema\n",
        "df_typed = spark.createDataFrame(data, schema)\n",
        "\n",
        "df_typed.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka4KgQTWIeYY",
        "outputId": "3abaa543-78fb-441d-e431-8b65c61238cc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = false)\n",
            " |-- name: string (nullable = false)\n",
            " |-- country: string (nullable = false)\n",
            " |-- amount: integer (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"name\", \"amount\") \\\n",
        "  .filter(df.amount > 100) \\\n",
        "  .orderBy(\"amount\", ascending=False) \\\n",
        "  .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSRoI73EI58A",
        "outputId": "c565c36d-cb92-4d5f-a976-56c5c01a3f94"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+\n",
            "|   name|amount|\n",
            "+-------+------+\n",
            "|    Eve|   300|\n",
            "|    Bob|   200|\n",
            "|Charlie|   150|\n",
            "+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum\n",
        "\n",
        "df.groupBy(\"country\").agg(sum(\"amount\").alias(\"total\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rye0byZpJCWT",
        "outputId": "6e0afc51-ea5b-4a87-9595-971a9e681c9e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|country|total|\n",
            "+-------+-----+\n",
            "|     FR|  350|\n",
            "|     UK|  450|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Observe the execution plan\n",
        "df.groupBy(\"country\").agg(sum(\"amount\")).explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZRESp9lJLQK",
        "outputId": "190bd839-a152-4234-96f2-0f86c63360eb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[country#2], functions=[sum(amount#3L)])\n",
            "   +- Exchange hashpartitioning(country#2, 200), ENSURE_REQUIREMENTS, [plan_id=87]\n",
            "      +- HashAggregate(keys=[country#2], functions=[partial_sum(amount#3L)])\n",
            "         +- Project [country#2, amount#3L]\n",
            "            +- Scan ExistingRDD[id#0L,name#1,country#2,amount#3L]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum, col # Ensure col is imported if not already\n",
        "\n",
        "# ✅ Filter FIRST, then groupBy (better)\n",
        "df.filter(df.amount > 100) \\\n",
        "  .groupBy(\"country\") \\\n",
        "  .agg(sum(\"amount\").alias(\"total\")) \\\n",
        "  .explain()\n",
        "\n",
        "# ❌ groupBy FIRST, then filter (worse - original intent, but with corrected syntax)\n",
        "df_grouped = df.groupBy(\"country\") \\\n",
        "               .agg(sum(\"amount\").alias(\"total\"))\n",
        "\n",
        "# The error was trying to filter on df.amount which is not in df_grouped.\n",
        "# This correction makes the second part runnable by filtering on the aggregated 'total' column.\n",
        "df_grouped.filter(col(\"total\") > 100) \\\n",
        "          .explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8S3aCi0UJSb3",
        "outputId": "9ba554c3-de9a-4a7c-a07b-fd257d0ea120"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[country#2], functions=[sum(amount#3L)])\n",
            "   +- Exchange hashpartitioning(country#2, 200), ENSURE_REQUIREMENTS, [plan_id=129]\n",
            "      +- HashAggregate(keys=[country#2], functions=[partial_sum(amount#3L)])\n",
            "         +- Project [country#2, amount#3L]\n",
            "            +- Filter (isnotnull(amount#3L) AND (amount#3L > 100))\n",
            "               +- Scan ExistingRDD[id#0L,name#1,country#2,amount#3L]\n",
            "\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Filter (isnotnull(total#74L) AND (total#74L > 100))\n",
            "   +- HashAggregate(keys=[country#2], functions=[sum(amount#3L)])\n",
            "      +- Exchange hashpartitioning(country#2, 200), ENSURE_REQUIREMENTS, [plan_id=150]\n",
            "         +- HashAggregate(keys=[country#2], functions=[partial_sum(amount#3L)])\n",
            "            +- Project [country#2, amount#3L]\n",
            "               +- Scan ExistingRDD[id#0L,name#1,country#2,amount#3L]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a temporary view\n",
        "df.createOrReplaceTempView(\"transactions\")\n",
        "\n",
        "# Now use pure SQL !\n",
        "result = spark.sql(\"\"\"\n",
        "    SELECT country, SUM(amount) as total\n",
        "    FROM transactions\n",
        "    GROUP BY country\n",
        "    ORDER BY total DESC\n",
        "\"\"\")\n",
        "\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OqDBUCAJd7u",
        "outputId": "45224861-d333-44ee-ee88-d5cdb3a941a0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|country|total|\n",
            "+-------+-----+\n",
            "|     UK|  450|\n",
            "|     FR|  350|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. DataFrame API\n",
        "df.groupBy(\"country\").agg(sum(\"amount\"))\n",
        "\n",
        "# 2. SQL on a temp view\n",
        "spark.sql(\"SELECT country, SUM(amount) FROM transactions GROUP BY country\")\n",
        "\n",
        "# 3. RDD (old way, much more verbose)\n",
        "# rdd.map(...).reduceByKey(...)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuqiltOvJilq",
        "outputId": "51da558a-2034-447a-ac21-85b7a24fbc65"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[country: string, sum(amount): bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}